{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting system model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: The_directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the working directory to the folder containing your notebook\n",
    "os.chdir(r\"The_directory")\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and exploring dataset...\n",
      "\n",
      "Dataset successfully loaded: C:\\models\\news_model.csv\n",
      "\n",
      "Dataset Overview:\n",
      "Total samples: 56290\n",
      "Total columns: 6\n",
      "\n",
      "Columns in dataset:\n",
      "['id', 'title', 'text', 'subject', 'date', 'label'] ...\n",
      "\n",
      "Label distribution (raw):\n",
      "label\n",
      "TRUE                                                                                                                                                                                                                                                                                                                                                       38871\n",
      "Fake                                                                                                                                                                                                                                                                                                                                                       17400\n",
      "fake                                                                                                                                                                                                                                                                                                                                                           3\n",
      " increased social welfare spending                                                                                                                                                                                                                                                                                                                             1\n",
      "2-Aug-16                                                                                                                                                                                                                                                                                                                                                       1\n",
      " an increasing number of smart news consumers came to realize that for at least the last century and half                                                                                                                                                                                                                                                      1\n",
      " but the newly circulated memo states that Tippit met with Oswald and Ruby at a nightclub a week prior to the shooting.An additional memo released suggested that the UK s Cambridge News received a warning call 25 minutes prior to JFK s death. This information was written in a memo by then FBI Deputy Director James Angleton to Hoover. However        1\n",
      " 21WIRE revealed that Nevada officials sought to increase their budget to thwart potential terror related activity                                                                                                                                                                                                                                             1\n",
      " consists of repeating that nothing is wrong                                                                                                                                                                                                                                                                                                                   1\n",
      " it was discovered that Frank s then boyfriend Stephen Gobie (who the US representative once solicited for an $80 dollar sexual encounter) had been running a prostitution ring out of Congressman Frank s Capitol Hill apartment   according to Gobie                                                                                                         1\n",
      " but also its corporate share prices                                                                                                                                                                                                                                                                                                                           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution (percentage):\n",
      "label\n",
      "TRUE                                                                                                                                                                                                                                                                                                                                                       69.064710\n",
      "Fake                                                                                                                                                                                                                                                                                                                                                       30.915746\n",
      "fake                                                                                                                                                                                                                                                                                                                                                        0.005330\n",
      " increased social welfare spending                                                                                                                                                                                                                                                                                                                          0.001777\n",
      "2-Aug-16                                                                                                                                                                                                                                                                                                                                                    0.001777\n",
      " an increasing number of smart news consumers came to realize that for at least the last century and half                                                                                                                                                                                                                                                   0.001777\n",
      " but the newly circulated memo states that Tippit met with Oswald and Ruby at a nightclub a week prior to the shooting.An additional memo released suggested that the UK s Cambridge News received a warning call 25 minutes prior to JFK s death. This information was written in a memo by then FBI Deputy Director James Angleton to Hoover. However     0.001777\n",
      " 21WIRE revealed that Nevada officials sought to increase their budget to thwart potential terror related activity                                                                                                                                                                                                                                          0.001777\n",
      " consists of repeating that nothing is wrong                                                                                                                                                                                                                                                                                                                0.001777\n",
      " it was discovered that Frank s then boyfriend Stephen Gobie (who the US representative once solicited for an $80 dollar sexual encounter) had been running a prostitution ring out of Congressman Frank s Capitol Hill apartment   according to Gobie                                                                                                      0.001777\n",
      " but also its corporate share prices                                                                                                                                                                                                                                                                                                                        0.001777\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values by column:\n",
      "         Missing Values  Percentage Missing\n",
      "title                 1            0.001777\n",
      "text                  2            0.003553\n",
      "subject              11            0.019542\n",
      "date                 11            0.019542\n",
      "label                 8            0.014212\n",
      "\n",
      "Text length statistics:\n",
      "count    56288.000000\n",
      "mean      2555.768903\n",
      "std       1846.209230\n",
      "min          5.000000\n",
      "25%       1530.000000\n",
      "50%       2177.000000\n",
      "75%       3069.000000\n",
      "max      32707.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Sample of shortest articles:\n",
      "48602     and is heavily funded by Democracy Alliance\n",
      "49984                                           Enjoy\n",
      "50160                                     Read moreWT\n",
      "50224    Maddow has a freak out at the 48 second mark\n",
      "50232                   Watch the hypocrisy in action\n",
      "Name: text, dtype: object\n",
      "\n",
      "Sample of longest articles:\n",
      "124    PETALING JAYA There appears to be little evide...\n",
      "323    More funding for facilities, teacher training ...\n",
      "700    Assemblyman lists multiple persistent issues r...\n",
      "722    THE current landscape of innovation and progre...\n",
      "900    Amidst global calls for safer online environme...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Duplicate analysis:\n",
      "Total duplicates: 1\n",
      "\n",
      "Data Quality Summary:\n",
      "1. Number of unique labels: 11\n",
      "2. Number of empty texts: 0\n",
      "3. Number of very short texts (<50 chars): 102\n",
      "4. Number of very long texts (>99th percentile): 563\n",
      "\n",
      "Initial data quality report saved. Ready for preprocessing phase.\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Enhanced Data Loading and Exploration\n",
    "print(\"Loading and exploring dataset...\")\n",
    "data_path = r\"C:\\models\\news_model.csv\" \n",
    "\n",
    "# Load data with better error handling\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "    print(f\"\\nDataset successfully loaded: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(\"\\nColumns in dataset:\")\n",
    "print(df.columns.tolist()[:10], \"...\") # Show first 10 columns\n",
    "\n",
    "# Examine label distribution\n",
    "print(\"\\nLabel distribution (raw):\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nLabel distribution (percentage):\")\n",
    "print(df['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check for data quality issues\n",
    "print(\"\\nMissing values by column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_pct = (missing_values / len(df)) * 100\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': missing_pct\n",
    "})\n",
    "print(missing_stats[missing_stats['Missing Values'] > 0])\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(\"\\nText length statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Show samples of very short and very long articles\n",
    "print(\"\\nSample of shortest articles:\")\n",
    "print(df[df['text_length'] < 50]['text'].head())\n",
    "print(\"\\nSample of longest articles:\")\n",
    "print(df[df['text_length'] > df['text_length'].quantile(0.99)]['text'].head())\n",
    "\n",
    "# Check for potential duplicates\n",
    "print(\"\\nDuplicate analysis:\")\n",
    "print(f\"Total duplicates: {df.duplicated(subset='text').sum()}\")\n",
    "\n",
    "# Data quality summary\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"1. Number of unique labels: {df['label'].nunique()}\")\n",
    "print(f\"2. Number of empty texts: {(df['text'].str.len() == 0).sum()}\")\n",
    "print(f\"3. Number of very short texts (<50 chars): {(df['text_length'] < 50).sum()}\")\n",
    "print(f\"4. Number of very long texts (>99th percentile): {(df['text_length'] > df['text_length'].quantile(0.99)).sum()}\")\n",
    "\n",
    "# Save initial data quality report\n",
    "quality_report = {\n",
    "    'total_samples': len(df),\n",
    "    'missing_values': df.isnull().sum().to_dict(),\n",
    "    'label_distribution': df['label'].value_counts().to_dict(),\n",
    "    'text_length_stats': df['text_length'].describe().to_dict(),\n",
    "    'duplicates': df.duplicated(subset='text').sum()\n",
    "}\n",
    "\n",
    "print(\"\\nInitial data quality report saved. Ready for preprocessing phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting enhanced preprocessing...\n",
      "\n",
      "Original label distribution:\n",
      "label\n",
      "TRUE                                                                                                                                                                                                                                                                                                                                                       38871\n",
      "Fake                                                                                                                                                                                                                                                                                                                                                       17400\n",
      "fake                                                                                                                                                                                                                                                                                                                                                           3\n",
      " increased social welfare spending                                                                                                                                                                                                                                                                                                                             1\n",
      "2-Aug-16                                                                                                                                                                                                                                                                                                                                                       1\n",
      " an increasing number of smart news consumers came to realize that for at least the last century and half                                                                                                                                                                                                                                                      1\n",
      " but the newly circulated memo states that Tippit met with Oswald and Ruby at a nightclub a week prior to the shooting.An additional memo released suggested that the UK s Cambridge News received a warning call 25 minutes prior to JFK s death. This information was written in a memo by then FBI Deputy Director James Angleton to Hoover. However        1\n",
      " 21WIRE revealed that Nevada officials sought to increase their budget to thwart potential terror related activity                                                                                                                                                                                                                                             1\n",
      " consists of repeating that nothing is wrong                                                                                                                                                                                                                                                                                                                   1\n",
      " it was discovered that Frank s then boyfriend Stephen Gobie (who the US representative once solicited for an $80 dollar sexual encounter) had been running a prostitution ring out of Congressman Frank s Capitol Hill apartment   according to Gobie                                                                                                         1\n",
      " but also its corporate share prices                                                                                                                                                                                                                                                                                                                           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After standardization:\n",
      "label\n",
      "True                                                                                                                                                                                                                                                                                                                                                      38871\n",
      "Fake                                                                                                                                                                                                                                                                                                                                                      17403\n",
      "increased social welfare spending                                                                                                                                                                                                                                                                                                                             1\n",
      "2-aug-16                                                                                                                                                                                                                                                                                                                                                      1\n",
      "an increasing number of smart news consumers came to realize that for at least the last century and half                                                                                                                                                                                                                                                      1\n",
      "but the newly circulated memo states that tippit met with oswald and ruby at a nightclub a week prior to the shooting.an additional memo released suggested that the uk s cambridge news received a warning call 25 minutes prior to jfk s death. this information was written in a memo by then fbi deputy director james angleton to hoover. however        1\n",
      "21wire revealed that nevada officials sought to increase their budget to thwart potential terror related activity                                                                                                                                                                                                                                             1\n",
      "consists of repeating that nothing is wrong                                                                                                                                                                                                                                                                                                                   1\n",
      "it was discovered that frank s then boyfriend stephen gobie (who the us representative once solicited for an $80 dollar sexual encounter) had been running a prostitution ring out of congressman frank s capitol hill apartment   according to gobie                                                                                                         1\n",
      "but also its corporate share prices                                                                                                                                                                                                                                                                                                                           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final label distribution:\n",
      "label\n",
      "True    38871\n",
      "Fake    17390\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "True    0.690905\n",
      "Fake    0.309095\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Shape after removing duplicates: (56261, 7)\n",
      "\n",
      "Training set size: 45008\n",
      "Testing set size: 11253\n",
      "\n",
      "Fitting TF-IDF vectorizer...\n",
      "Vocabulary size: 5000\n",
      "Training set shape after vectorization: (45008, 5000)\n",
      "\n",
      "Applying SMOTE to balance classes...\n",
      "Class distribution after SMOTE:\n",
      "Fake: 31096\n",
      "True: 31096\n",
      "\n",
      "Preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Enhanced Data Preprocessing with Better Label Handling\n",
    "print(\"\\nStarting enhanced preprocessing...\")\n",
    "\n",
    "# First, show original data distribution\n",
    "print(\"\\nOriginal label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Standardize labels more carefully\n",
    "df['label'] = df['label'].str.strip().str.lower()\n",
    "df['label'] = df['label'].replace({\n",
    "    'true': 'True',\n",
    "    'fake': 'Fake',\n",
    "    'true ': 'True',\n",
    "    'fake ': 'Fake',\n",
    "    'TRUE': 'True',\n",
    "    'FALSE': 'Fake'\n",
    "})\n",
    "\n",
    "# Print intermediate distribution\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Clean data\n",
    "df = df[~df['text'].isna()]  # Remove rows with no text\n",
    "df = df[df['text'].str.len() > 10]  # Remove very short texts\n",
    "\n",
    "# Keep only valid labels\n",
    "valid_labels = ['True', 'Fake']\n",
    "df = df[df['label'].isin(valid_labels)]\n",
    "\n",
    "# Print final distribution\n",
    "print(\"\\nFinal label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Remove duplicate articles\n",
    "df = df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"\\nShape after removing duplicates: {df.shape}\")\n",
    "\n",
    "\n",
    "# Convert labels to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['label'])\n",
    "X = df['text']\n",
    "\n",
    "# Split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    max_df=0.7,\n",
    "    min_df=2,  # Remove very rare words\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Training set shape after vectorization: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Apply SMOTE for balance\n",
    "print(\"\\nApplying SMOTE to balance classes...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_tfidf_balanced, y_train_balanced = smote.fit_resample(\n",
    "    X_train_tfidf, y_train\n",
    ")\n",
    "\n",
    "# Print class distribution after SMOTE\n",
    "unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "for label, count in zip(['Fake', 'True'], counts):\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Model Training and Evaluation Function\n",
    "def train_evaluate_model(model, name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training PassiveAggressive...\n",
      "PassiveAggressive Results:\n",
      "Accuracy: 0.9965\n",
      "F1 Score: 0.9975\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3460   18]\n",
      " [  21 7754]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3478\n",
      "           1       1.00      1.00      1.00      7775\n",
      "\n",
      "    accuracy                           1.00     11253\n",
      "   macro avg       1.00      1.00      1.00     11253\n",
      "weighted avg       1.00      1.00      1.00     11253\n",
      "\n",
      "\n",
      "Training NaiveBayes...\n",
      "NaiveBayes Results:\n",
      "Accuracy: 0.9796\n",
      "F1 Score: 0.9851\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3432   46]\n",
      " [ 184 7591]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      3478\n",
      "           1       0.99      0.98      0.99      7775\n",
      "\n",
      "    accuracy                           0.98     11253\n",
      "   macro avg       0.97      0.98      0.98     11253\n",
      "weighted avg       0.98      0.98      0.98     11253\n",
      "\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest Results:\n",
      "Accuracy: 0.9917\n",
      "F1 Score: 0.9940\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3409   69]\n",
      " [  24 7751]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      3478\n",
      "           1       0.99      1.00      0.99      7775\n",
      "\n",
      "    accuracy                           0.99     11253\n",
      "   macro avg       0.99      0.99      0.99     11253\n",
      "weighted avg       0.99      0.99      0.99     11253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Train Individual Models\n",
    "results = {}\n",
    "\n",
    "# PassiveAggressive\n",
    "pa_model = PassiveAggressiveClassifier(max_iter=1000)\n",
    "results['PassiveAggressive'] = train_evaluate_model(\n",
    "    pa_model, 'PassiveAggressive',\n",
    "    X_train_tfidf_balanced, y_train_balanced,\n",
    "    X_test_tfidf, y_test\n",
    ")\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "results['NaiveBayes'] = train_evaluate_model(\n",
    "    nb_model, 'NaiveBayes',\n",
    "    X_train_tfidf_balanced, y_train_balanced,\n",
    "    X_test_tfidf, y_test\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "results['RandomForest'] = train_evaluate_model(\n",
    "    rf_model, 'RandomForest',\n",
    "    X_train_tfidf_balanced, y_train_balanced,\n",
    "    X_test_tfidf, y_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Voting...\n",
      "Voting Results:\n",
      "Accuracy: 0.9956\n",
      "F1 Score: 0.9968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3449   29]\n",
      " [  21 7754]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3478\n",
      "           1       1.00      1.00      1.00      7775\n",
      "\n",
      "    accuracy                           1.00     11253\n",
      "   macro avg       1.00      0.99      0.99     11253\n",
      "weighted avg       1.00      1.00      1.00     11253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Train Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('pa', PassiveAggressiveClassifier()),\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "results['Voting'] = train_evaluate_model(\n",
    "    voting_clf, 'Voting',\n",
    "    X_train_tfidf_balanced, y_train_balanced,\n",
    "    X_test_tfidf, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 0.9958\n",
      "F1 Score: 0.9970\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3452   26]\n",
      " [  21 7754]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3478\n",
      "           1       1.00      1.00      1.00      7775\n",
      "\n",
      "    accuracy                           1.00     11253\n",
      "   macro avg       1.00      0.99      1.00     11253\n",
      "weighted avg       1.00      1.00      1.00     11253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "voting_clf.fit(X_train_tfidf_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = voting_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"\\nVoting Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to: C:\\models\\best_news_model.pkl\n"
     ]
    }
   ],
   "source": [
    "save_path = r\\models\\best_news_model.pkl\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump((voting_clf, tfidf), f)\n",
    "\n",
    "print(f\"\\nModel saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
